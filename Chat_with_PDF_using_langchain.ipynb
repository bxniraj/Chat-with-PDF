{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This the simplest method to extract text from pdf and chat with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz1wHcxxgLvJ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain pinecone-client transformers accelerate bitsandbytes PyPDF2 sentence_transformers openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLmHG1D9gOxU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pinecone\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone, FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "torch.set_default_device('cuda')\n",
        "# model for hf embeddings\n",
        "model_name = \"bert-large-uncased\" # use any model from huggingface\n",
        "\n",
        "# openai api key\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# hf api key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''\n",
        "\n",
        "# pinecone index\n",
        "index_name = 'langchain'\n",
        "pinecone.init(\n",
        "\tapi_key='',\n",
        "\tenvironment='gcp-starter'\n",
        ")\n",
        "\n",
        "# for deleting the index in pinecone\n",
        "# if index_name in pinecone.list_indexes():\n",
        "#     pinecone.delete_index(index_name)\n",
        "\n",
        "\n",
        "# for creating the index in pinecone\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=1024\n",
        "    )\n",
        "\n",
        "reader = PdfReader('/content/comp.pdf') # location of pdf\n",
        "\n",
        "# extract text\n",
        "raw_text = ''\n",
        "for i, page in enumerate(reader.pages):\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text\n",
        "\n",
        "# text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=60)\n",
        "chunks = text_splitter.split_text(raw_text)\n",
        "\n",
        "# hf embedding\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "vector_store = Pinecone.from_texts(texts=chunks, embedding=hf_embeddings, index_name=index_name)\n",
        "\n",
        "# # open_ai embedding\n",
        "# openai_embeddings = OpenAIEmbeddings()\n",
        "# vector_store = FAISS.from_texts(chunks, openai_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Znv2qNxgTCx"
      },
      "outputs": [],
      "source": [
        "# hf llm model\n",
        "model_id = \"\"\n",
        "\n",
        "hf_llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=model_id,\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 1024},\n",
        "    device=0 # if device set to 0 means GPU\n",
        ")\n",
        "\n",
        "# openai llm model\n",
        "# openai_llm = ChatOpenAI(\n",
        "#     model=\"gpt-3.5-turbo\",\n",
        "#     max_tokens=1024,\n",
        "# )\n",
        "\n",
        "# memory chain for chat history\n",
        "memory = ConversationBufferMemory(\n",
        "          memory_key=\"chat_history\",\n",
        "          return_messages=True,\n",
        ")\n",
        "\n",
        "# retrival chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=hf_llm,\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpUNCS6UgVm2"
      },
      "outputs": [],
      "source": [
        "question = 'what is program'\n",
        "response = qa_chain(question)\n",
        "print(\"Response:\", response['answer'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
