{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This convert pdf pages to an image and then extract text from it by using OCR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEd0NUQQ3gvl"
      },
      "outputs": [],
      "source": [
        "!pip install langchain pinecone-client transformers PyPDF2 sentence_transformers openai tiktoken torch pytesseract pdf2image PyMuPDF Pillow unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i7CZO8ciK2a"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9mFXsZp885k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import torch\n",
        "import pinecone\n",
        "import fitz\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores.pinecone import Pinecone\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import HuggingFacePipeline, HuggingFaceHub\n",
        "\n",
        "\n",
        "# GPU\n",
        "torch.set_default_device('cuda')\n",
        "\n",
        "# hf_hub\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''\n",
        "\n",
        "# pinecone_api\n",
        "index_name = 'langchain'\n",
        "pinecone.init(\n",
        "\tapi_key='',\n",
        "\tenvironment='gcp-starter'\n",
        ")\n",
        "\n",
        "# delete existing index in pinecone\n",
        "if index_name in pinecone.list_indexes():\n",
        "    pinecone.delete_index(name=index_name)\n",
        "\n",
        "# create new index in pinecone\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(name=index_name, metric=\"cosine\", dimension=1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "408EfmfHwUsV"
      },
      "outputs": [],
      "source": [
        "pdf = '' #example.pdf\n",
        "doc = fitz.open(pdf)\n",
        "\n",
        "# method to extract texts from pages\n",
        "all_texts = ''\n",
        "for page_number in range(doc.page_count):\n",
        "    page = doc[page_number]\n",
        "    pix = page.get_pixmap(matrix=fitz.Matrix(10, 10))\n",
        "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "    extracted_text = pytesseract.image_to_string(image)\n",
        "    all_texts += extracted_text.replace('\\n\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDEPOdOLwXi4"
      },
      "outputs": [],
      "source": [
        "# covert to chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=20)\n",
        "chunks = text_splitter.split_text(all_texts)\n",
        "\n",
        "# id\n",
        "chunk_ids = [\"langchain-\" + str(uuid.uuid4()) for _ in chunks]\n",
        "\n",
        "# embedding\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"bert-large-cased\")\n",
        "vector_store = Pinecone.from_texts(ids=chunk_ids, texts=chunks, embedding=hf_embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e5axs-a2rLM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"max_new_tokens\": 1024,\n",
        "    \"repetition_penalty\": 1.1,\n",
        "    \"temperature\": 0.9,\n",
        "    \"top_k\": 10,\n",
        "    \"top_p\": 0.9,\n",
        "}\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"bigscience/bloom\",  # model to use from huggingface\n",
        "    model_kwargs=config)\n",
        "\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True)\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=vector_store.as_retriever(search_type='similarity', search_kwargs={\"k\": 4}),\n",
        "            memory=memory,\n",
        "            chain_type='stuff',\n",
        "            return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_Kb-C8l2r3M"
      },
      "outputs": [],
      "source": [
        "questions = ''\n",
        "response = chain(questions)\n",
        "response['result'].replace('\\n',' ')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
